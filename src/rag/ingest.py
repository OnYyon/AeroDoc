from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import chromadb
from typing import List, Dict, Optional
from pathlib import Path
import markitdown
import hashlib
from datetime import datetime
import uuid

from src.rag.config import CHROMA_PATH, CHUNK_SIZE, CHUNK_OVERLAP, COLLECTION_NAME, EMBEDDING_MODEL, DOCUMENTS_DIR, SUPPORTED_FORMATS
from src.rag.logger import logger
from src.rag.models import Document, DocumentMetadata, DocumentChunk


def get_file_checksum(file_path: str) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Å—É–º–º—É —Ñ–∞–π–ª–∞."""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


def load_document_content(file_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤."""
    path = Path(file_path)
    
    try:
        if path.suffix.lower() == ".pdf":
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ PDF: {path.name}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º markitdown –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –≤ Markdown
            result = markitdown.MarkItDown().convert(str(path))
            # result —ç—Ç–æ DocumentConverterResult —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º text_content
            text = result.text_content if hasattr(result, 'text_content') else str(result)
            return text if text else ""
        elif path.suffix.lower() == ".txt":
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ TXT: {path.name}")
            with open(str(path), "r", encoding="utf-8") as f:
                return f.read()
        elif path.suffix.lower() == ".md":
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ Markdown: {path.name}")
            with open(str(path), "r", encoding="utf-8") as f:
                return f.read()
        else:
            logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {path.suffix}")
            return ""
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {path}: {str(e)}")
        return ""


def split_document(content: str, source: str, document_id: str) -> List[DocumentChunk]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã DocumentChunk."""
    if not content:
        logger.warning(f"–ü—É—Å—Ç–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è {source}")
        return []
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    chunks = splitter.split_text(content)
    
    document_chunks = []
    for i, chunk_content in enumerate(chunks):
        chunk = DocumentChunk(
            id=str(uuid.uuid4()),
            document_id=document_id,
            content=chunk_content,
            metadata={"chunk_index": i, "source": source},
            page_number=None,
            section=None
        )
        document_chunks.append(chunk)
    
    logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(document_chunks)} —á–∞–Ω–∫–æ–≤: {source}")
    return document_chunks


def create_document_metadata(file_path: str) -> DocumentMetadata:
    """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    path = Path(file_path)
    
    return DocumentMetadata(
        title=path.stem,
        version="1.0",
        author="System",
        created_date=datetime.fromtimestamp(path.stat().st_ctime),
        modified_date=datetime.fromtimestamp(path.stat().st_mtime),
        file_type=path.suffix.lower(),
        size=path.stat().st_size,
        checksum=get_file_checksum(str(path)),
        source=str(path)
    )


def ingest_document(file_path: str) -> Optional[Document]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é."""
    path = Path(file_path)
    
    if not path.exists():
        logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
        return None
    
    if path.suffix.lower() not in SUPPORTED_FORMATS:
        logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {path.suffix}")
        return None
    
    try:
        document_id = str(uuid.uuid4())
        content = load_document_content(str(path))
        
        if not content:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑: {path}")
            return None
        
        metadata = create_document_metadata(str(path))
        chunks = split_document(content, str(path), document_id)
        
        if not chunks:
            logger.warning(f"–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {path}")
            return None
        
        document = Document(
            id=document_id,
            metadata=metadata,
            chunks=chunks
        )
        
        logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {path.name}")
        return document
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {path}: {str(e)}")
        return None


def get_document_hash_from_db(source: str) -> Optional[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ö–µ—à –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –ë–î –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É (–ø–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–µ—Ä–≤—ã–π)."""
    try:
        client = chromadb.PersistentClient(path=str(CHROMA_PATH))
        collection = client.get_collection(name=COLLECTION_NAME)
        
        # –ò—â–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        results = collection.get(
            where={"source": source},
            limit=1  # –ù—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ö–µ—à–∞
        )
        
        if results and results["metadatas"] and len(results["metadatas"]) > 0:
            file_hash = results["metadatas"][0].get("file_hash")
            if file_hash:
                logger.debug(f"–ù–∞–π–¥–µ–Ω —Ö–µ—à –≤ –ë–î –¥–ª—è {Path(source).name}: {file_hash}")
                return file_hash
        logger.debug(f"–•–µ—à –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î –¥–ª—è {source}")
        return None
    except Exception as e:
        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ö–µ—à–∞ –∏–∑ –ë–î: {str(e)}")
        return None


def delete_document_from_db(source: str):
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –ë–î –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É."""
    try:
        client = chromadb.PersistentClient(path=str(CHROMA_PATH))
        collection = client.get_collection(name=COLLECTION_NAME)
        
        # –ò—â–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        results = collection.get(
            where={"source": source}
        )
        
        if results and results["ids"]:
            collection.delete(ids=results["ids"])
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(results['ids'])} —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source}")
    except Exception as e:
        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {str(e)}")


def ingest_documents_to_db(documents: List[Document], force_update: bool = False):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ Chroma DB (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ö–µ—à–µ–π —É–∂–µ —Å–¥–µ–ª–∞–Ω–∞ –≤ ingest_from_directory)."""
    if not documents:
        logger.warning("–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—É—Å—Ç")
        return
    
    try:
        model = SentenceTransformer(EMBEDDING_MODEL)
        client = chromadb.PersistentClient(path=str(CHROMA_PATH))
        collection = client.get_or_create_collection(name=COLLECTION_NAME)
        
        total_chunks = 0
        
        for document in documents:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –ë–î
            chunk_contents = [chunk.content for chunk in document.chunks]
            logger.info(f"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {len(chunk_contents)} —á–∞–Ω–∫–æ–≤ –¥–ª—è {Path(document.metadata.source).name}...")
            embeddings = model.encode(chunk_contents)
            
            for chunk, embedding in zip(document.chunks, embeddings):
                metadata = {
                    "source": document.metadata.source,
                    "title": document.metadata.title,
                    "file_type": document.metadata.file_type,
                    "file_hash": document.metadata.checksum,
                    "chunk_index": chunk.metadata.get("chunk_index", 0)
                }
                
                collection.add(
                    documents=[chunk.content],
                    embeddings=[embedding.tolist()],
                    metadatas=[metadata],
                    ids=[chunk.id]
                )
                total_chunks += 1
        
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_chunks} —á–∞–Ω–∫–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é '{COLLECTION_NAME}'")
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î: {str(e)}")
        raise


def ingest_from_directory(directory: Optional[Path] = None, force_update: bool = False) -> List[Document]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–æ —Ö–µ—à–∞–º)."""
    if directory is None:
        directory = DOCUMENTS_DIR
    
    directory = Path(directory)
    
    if not directory.exists():
        logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory}")
        return []
    
    logger.info(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    collection_exists = False
    try:
        client = chromadb.PersistentClient(path=str(CHROMA_PATH))
        collection = client.get_collection(name=COLLECTION_NAME)
        logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞: {collection.count()} —á–∞–Ω–∫–æ–≤ –≤ –ë–î")
        collection_exists = True
    except Exception as e:
        logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        collection_exists = False
    
    documents_to_process = []
    skipped_files = 0
    
    # –ü–ï–†–í–´–ô –ü–†–û–•–û–î: —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ö–µ—à–∞–º –ë–ï–ó –∑–∞–≥—Ä—É–∑–∫–∏
    for file_format in SUPPORTED_FORMATS:
        for file_path in directory.glob(f"*{file_format}"):
            try:
                file_name = file_path.name
                file_path_str = str(file_path)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
                current_hash = get_file_checksum(file_path_str)
                
                # –ü–æ–ª—É—á–∞–µ–º —Ö–µ—à –∏–∑ –ë–î –µ—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                if collection_exists:
                    db_hash = get_document_hash_from_db(file_path_str)
                else:
                    db_hash = None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏–∫—É
                if db_hash is not None:
                    # –§–∞–π–ª –µ—Å—Ç—å –≤ –ë–î
                    if db_hash == current_hash and not force_update:
                        # –§–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        logger.info(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫ (—É–∂–µ –≤ –ë–î): {file_name} [—Ö–µ—à —Å–æ–≤–ø–∞–¥–∞–µ—Ç]")
                        skipped_files += 1
                        continue
                    else:
                        # –§–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è - –æ–±–Ω–æ–≤–ª—è–µ–º
                        logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ (—Ö–µ—à –∏–∑–º–µ–Ω–∏–ª—Å—è): {file_name}")
                        delete_document_from_db(file_path_str)
                else:
                    # –§–∞–π–ª–∞ –Ω–µ—Ç –≤ –ë–î - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ–∞–π–ª
                    logger.info(f"‚ûï –ù–æ–≤—ã–π —Ñ–∞–π–ª: {file_name}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
                documents_to_process.append(file_path_str)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ö–µ—à–∞ {file_path.name}: {str(e)}")
                continue
    
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: –ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped_files} —Ñ–∞–π–ª–æ–≤, –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ {len(documents_to_process)}")
    
    # –í–¢–û–†–û–ô –ü–†–û–•–û–î: –∑–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Ñ–∞–π–ª—ã
    documents = []
    for file_path_str in documents_to_process:
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {Path(file_path_str).name}")
        document = ingest_document(file_path_str)
        if document:
            documents.append(document)
    
    if documents:
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –Ω–∞—á–∏–Ω–∞—é –∏–Ω–∂–µ—Å—Ç–∏—é –≤ –ë–î...")
        ingest_documents_to_db(documents, force_update=False)
    else:
        if skipped_files > 0:
            logger.info(f"‚úÖ –í—Å–µ {skipped_files} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —É–∂–µ –≤ –ë–î —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ —Ö–µ—à–∞–º–∏")
        else:
            logger.warning(f"–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {directory}")
    
    return documents
